{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600631747145",
   "display_name": "Python 3.7.9 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import Python libraries for analyzing photos of space rocks\n",
    "\n",
    "Now that we have all of the libraries downloaded, we can begin importing them into Jupyter Notebook file. We'll begin by opening or creating a Jupyter Notebook file.\n",
    "\n",
    "Next, we import Matplotlib to help us plot our data:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "Then, we import Numpy, a library we'lll use to process large numerical matrixes (images):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "Import PyTorch to train and process deep learning and AI models:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "Import torchvision (part of pyTorch) to process images and manipulate them (crop, resize):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "source": [
    "Import Python Imagin Library (PIL) to visualize images:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "source": [
    "Finally, we add two libraries that ensure that the plots are shown inline and in high resolution:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "source": [
    "# Import and clean data about photos of space rocks\n",
    "\n",
    "Now that we know about cleaning and separating the data, we can actually apply these principles to our rock classification project.\n",
    "\n",
    "Let's start by dowloading all the data we have about rock images. Then, we'll put it in the same folder as you Jupyter Notebook file. Go to [this Azure Blob storage](https://nasadata.blob.core.windows.net/nasarocks/Data.zip) and download the *Data.zip* folder. Unzop it and put it in the same folder as your Jupyter Notebook file.\n",
    "\n",
    "Because our photos of rocks come in different sizes (small, medium, and large), we crop the imgaes so that they are the same size(22 x 224 pixels). We resize the images because computers expcet images to be same size. If images vary in size, they're not as easy for the computer to process.\n",
    "\n",
    "We resize the images in the first part of the code. At the bottom of the code, you can see that we separate the data into a training variable and a testing variable.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Basalt', 'Highland']\n"
    }
   ],
   "source": [
    "# Tells the machine what folder contains the image data.\n",
    "data_dir = './data'\n",
    "\n",
    "# Function to read the data; crop and resize the images; and then split it into test an train chunks.\n",
    "\n",
    "def load_split_train_test(datadir, valid_size = .2):\n",
    "# This line of code transforms the images.\n",
    "    train_transforms = transforms.Compose([\n",
    "                                        transforms.RandomResizedCrop(224),\n",
    "                                        transforms.Resize(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "                                        transforms.RandomResizedCrop(224),\n",
    "                                        transforms.Resize(224),\n",
    "                                        transforms.ToTensor(),\n",
    "\n",
    "                                        ])\n",
    "    train_data = datasets.ImageFolder(datadir, transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(datadir, transform=test_transforms)\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
    "    testloader = DataLoader(test_data, sampler=test_sampler, batch_size=16)\n",
    "    return trainloader, testloader\n",
    "\n",
    "# We're using 20% of data for testing.\n",
    "\n",
    "trainloader, testloader = load_split_train_test(data_dir, .2)\n",
    "print(trainloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}